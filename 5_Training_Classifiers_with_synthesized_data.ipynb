{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21e69250d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt \n",
    "from PIL import Image\n",
    "from utility import *\n",
    "from model import *\n",
    "from torchsummary import summary\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "MODEL_FILE_DIRC_SateLight = MODEL_FILE_DIRC + \"/SateLight_synthesized\"\n",
    "MODEL_FILE_DIRC_CNN       = MODEL_FILE_DIRC + \"/CNN_synthesized\"\n",
    "os.makedirs(MODEL_FILE_DIRC_CNN, exist_ok=True)\n",
    "os.makedirs(MODEL_FILE_DIRC_SateLight, exist_ok=True)\n",
    "\n",
    "MAX_COUNT_F1_SCORE    = 10\n",
    "torch.manual_seed(3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Preparation\n",
    "* Convert the data from csv to Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data from EEG_csv/eeg1.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(252, 1280, 19)\n",
      "EEG1 has 252 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg2.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(43, 1280, 19)\n",
      "EEG2 has 43 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg3.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(88, 1280, 19)\n",
      "EEG3 has 88 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg4.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(245, 1280, 19)\n",
      "EEG4 has 245 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg5.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(236, 1280, 19)\n",
      "EEG5 has 236 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg6.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (481920, 19)\n",
      "Data with spike   : (465280, 19)\n",
      "Data without spike: (16640, 19)\n",
      "Data after  split into window: (376, 1280, 19)\n",
      "Labels: (376,)\n",
      "Num spike: 13\n",
      "EEG6 has 376 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg7.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(311, 1280, 19)\n",
      "EEG7 has 311 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg8.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(243, 1280, 19)\n",
      "EEG8 has 243 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg9.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(322, 1280, 19)\n",
      "EEG9 has 322 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg10.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(422, 1280, 19)\n",
      "EEG10 has 422 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg11.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(126, 1280, 19)\n",
      "EEG11 has 126 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg12.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(48, 1280, 19)\n",
      "EEG12 has 48 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg13.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(182, 1280, 19)\n",
      "EEG13 has 182 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg14.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(217, 1280, 19)\n",
      "EEG14 has 217 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg15.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(319, 1280, 19)\n",
      "EEG15 has 319 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg16.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(58, 1280, 19)\n",
      "EEG16 has 58 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg17.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(208, 1280, 19)\n",
      "EEG17 has 208 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg18.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(313, 1280, 19)\n",
      "EEG18 has 313 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg19.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (413568, 19)\n",
      "Data with spike   : (411008, 19)\n",
      "Data without spike: (2560, 19)\n",
      "Data after  split into window: (323, 1280, 19)\n",
      "Labels: (323,)\n",
      "Num spike: 2\n",
      "EEG19 has 323 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg20.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(255, 1280, 19)\n",
      "EEG20 has 255 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg21.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(363, 1280, 19)\n",
      "EEG21 has 363 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg22.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(15, 1280, 19)\n",
      "EEG22 has 15 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg23.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(308, 1280, 19)\n",
      "EEG23 has 308 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg24.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (386560, 19)\n",
      "Data with spike   : (385280, 19)\n",
      "Data without spike: (1280, 19)\n",
      "Data after  split into window: (302, 1280, 19)\n",
      "Labels: (302,)\n",
      "Num spike: 1\n",
      "EEG24 has 302 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg25.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(426, 1280, 19)\n",
      "EEG25 has 426 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg26.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(237, 1280, 19)\n",
      "EEG26 has 237 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg27.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(147, 1280, 19)\n",
      "EEG27 has 147 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg28.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(173, 1280, 19)\n",
      "EEG28 has 173 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg29.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(29, 1280, 19)\n",
      "EEG29 has 29 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg30.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(300, 1280, 19)\n",
      "EEG30 has 300 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg31.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (381312, 19)\n",
      "Data with spike   : (341632, 19)\n",
      "Data without spike: (39680, 19)\n",
      "Data after  split into window: (297, 1280, 19)\n",
      "Labels: (297,)\n",
      "Num spike: 31\n",
      "EEG31 has 297 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg32.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(254, 1280, 19)\n",
      "EEG32 has 254 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg33.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(65, 1280, 19)\n",
      "EEG33 has 65 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg34.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(264, 1280, 19)\n",
      "EEG34 has 264 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg35.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(297, 1280, 19)\n",
      "EEG35 has 297 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg36.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (344448, 19)\n",
      "Data with spike   : (313728, 19)\n",
      "Data without spike: (30720, 19)\n",
      "Data after  split into window: (269, 1280, 19)\n",
      "Labels: (269,)\n",
      "Num spike: 24\n",
      "EEG36 has 269 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg37.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(300, 1280, 19)\n",
      "EEG37 has 300 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg38.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(94, 1280, 19)\n",
      "EEG38 has 94 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg39.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (55296, 19)\n",
      "Data with spike   : (54016, 19)\n",
      "Data without spike: (1280, 19)\n",
      "Data after  split into window: (43, 1280, 19)\n",
      "Labels: (43,)\n",
      "Num spike: 1\n",
      "EEG39 has 43 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg40.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(291, 1280, 19)\n",
      "EEG40 has 291 windows of data \n",
      "\n",
      "\n",
      "> > > Train    data  has shape: torch.Size([6345, 19, 1280]) when duration = 10 seconds\n",
      "> > > Data after DWT has shape: torch.Size([6345, 19, 1282])\n",
      "> > > Label              shape: torch.Size([6345])\n",
      "Number of training data: 8345\n",
      "Number of spike in training data: tensor(2050)\n",
      "CPU times: total: 2min 5s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eeg_num_list = list(range(1,41))\n",
    "datasets, datasets_label, datasets_DWT = get_dataloader(eeg_num_list, get_dataloader=False, shuffle=False)\n",
    "    \n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_label,   valid_label,   test_label   = datasets_label\n",
    "train_DWT,     valid_DWT,     test_DWT     = datasets_DWT\n",
    "\n",
    "# Load the synthesized data and process it (copy from Evaluate Diffusion Model)\n",
    "df                  = pd.read_csv(\"synthesized_data.csv\")  # Shape=(num_data, num_channel)\n",
    "synthesized_data = torch.tensor(df.values)\n",
    "synthesized_data = synthesized_data.view(synthesized_data.shape[0]//(DURATION*NEW_FREQUENCY),-1 , len(AVE_CHANNELS_NAME)) # Shape=(bs, num_signal, num_channel)\n",
    "synthesized_data = synthesized_data.permute(0,2,1)\n",
    "synthesized_data = synthesized_data.type(torch.float32) \n",
    "\n",
    "\n",
    "# Get the label and data after discrete wavelet transform\n",
    "synthesized_data_label = torch.full((synthesized_data.shape[0],), 1, dtype=torch.int8)\n",
    "synthesized_data_DWT   = pywt.wavedec(synthesized_data, 'db1')                         # Apply DWT\n",
    "synthesized_data_DWT   = np.concatenate(synthesized_data_DWT, axis=2, dtype=np.float32)# Concatenate all DWT data\n",
    "synthesized_data_DWT   = torch.from_numpy(synthesized_data_DWT)\n",
    "\n",
    "# Concatenate synthesized data and training data\n",
    "train_dataset = torch.cat([train_dataset, synthesized_data], axis=0) \n",
    "train_label   = torch.cat([train_label, synthesized_data_label], axis=0)\n",
    "train_DWT     = torch.cat([train_DWT, synthesized_data_DWT], axis=0)\n",
    "\n",
    "print(\"Number of training data:\", train_dataset.shape[0])\n",
    "print(\"Number of spike in training data:\", (train_label==1).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of data and Place it into dataloader\n",
    "num_train_data = train_dataset.shape[0]\n",
    "num_valid_data = valid_dataset.shape[0]\n",
    "num_test_data  = test_dataset.shape[0]\n",
    "train_data = DataLoader(dataset = Dataset_Class1(train_dataset, train_label), \n",
    "                        batch_size = BATCH_SIZE, shuffle = True, num_workers=1)\n",
    "valid_data = DataLoader(dataset = Dataset_Class1(valid_dataset, valid_label), \n",
    "                        batch_size = BATCH_SIZE, shuffle = True, num_workers=1)\n",
    "test_data  = DataLoader(dataset = Dataset_Class1(test_dataset, test_label), \n",
    "                        batch_size = BATCH_SIZE, shuffle = True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classification_model_dict(model, MODEL_FILE_DIRC, model_name):\n",
    "    list_model = os.listdir(MODEL_FILE_DIRC) \n",
    "    if len(list_model) > 0:    # Load the latest trained model\n",
    "        if os.path.exists(f\"{MODEL_FILE_DIRC}/{model_name}_best.pt\"):\n",
    "            state_dict_loaded    = torch.load(f\"{MODEL_FILE_DIRC}/{model_name}_best.pt\")\n",
    "            prev_best_valid_f1   = state_dict_loaded[\"valid_f1_score\"]\n",
    "            prev_best_valid_loss = state_dict_loaded[\"valid_loss\"]\n",
    "        list_model.remove(f\"{model_name}_best.pt\")\n",
    "        num_list   = [int(model_dir[model_dir.rindex(\"_\") +1: model_dir.rindex(\".\")]) for model_dir in list_model if model_dir.endswith(\".pt\")]\n",
    "        num_max    = np.max(num_list)\n",
    "        \n",
    "        state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC}/{model_name}_{num_max}.pt\")\n",
    "        model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "        EPOCH_START = state_dict_loaded[\"epoch\"] + 1\n",
    "        \n",
    "        print(f\"The model has been loaded from the file '{model_name}_{num_max}.pt'\")\n",
    "\n",
    "        if os.path.exists(f\"{MODEL_FILE_DIRC}/Loss.csv\"):\n",
    "            df = pd.read_csv(f\"{MODEL_FILE_DIRC}/Loss.csv\")\n",
    "            df = df.iloc[:EPOCH_START-1, :]\n",
    "            print(f\"The dataframe that record the loss have been loaded from {MODEL_FILE_DIRC}/Loss.csv\")\n",
    "\n",
    "    else:\n",
    "        EPOCH_START            = 1\n",
    "        prev_best_valid_f1     = -1\n",
    "        prev_best_valid_loss   = 10000\n",
    "        df                     = pd.DataFrame(columns = [\"Train Loss\", \"Valid Loss\"] + \\\n",
    "                                                         flatten_concatenation([[f\"Train {metric}\", f\"Valid {metric}\"] for metric in [\"precision\", \"accuracy\", \"f1_score\", \"recall\"]]) )\n",
    "    return model, df, EPOCH_START, prev_best_valid_f1, prev_best_valid_loss\n",
    "\n",
    "def start_classification_model_training(EPOCH_START, NUM_EPOCHS_CLASSIFIER, \n",
    "                                        model, MODEL_FILE_DIRC, model_name,  \n",
    "                                        df, prev_best_valid_f1,prev_best_valid_loss,\n",
    "                                        train_data, num_train_data, \n",
    "                                        valid_data, num_valid_data, \n",
    "                                        scheduler, optimizer, device):\n",
    "    count = 0\n",
    "    for epoch in range(EPOCH_START, NUM_EPOCHS_CLASSIFIER):\n",
    "        \n",
    "            ## 1. Training\n",
    "            model.train()\n",
    "            train_loss, train_metric = train_classifier(model, train_data, device, num_train_data, optimizer, LOSS_POS_WEIGHT=torch.tensor([3])) \n",
    "            \n",
    "            ## 2. Evaluating\n",
    "            model.eval()\n",
    "            valid_loss, valid_metric = evaluate_classifier(model, valid_data, device, num_valid_data, LOSS_POS_WEIGHT=torch.tensor([3]))  \n",
    "            \n",
    "            ## 3. Show the result\n",
    "            list_data       = [train_loss, valid_loss]\n",
    "            for key in [\"precision\", \"accuracy\", \"f1_score\", \"recall\"]:\n",
    "                list_data.append(train_metric[key])\n",
    "                list_data.append(valid_metric[key])\n",
    "            df.loc[len(df)] = list_data\n",
    "            \n",
    "            print_log(f\"> > > Epoch     : {epoch}\", MODEL_FILE_DIRC)\n",
    "            print_log(f\"Train {'loss':<10}: {train_loss}\", MODEL_FILE_DIRC)\n",
    "            print_log(f\"Valid {'loss':<10}: {valid_loss}\", MODEL_FILE_DIRC)\n",
    "            for key in [\"precision\", \"accuracy\", \"f1_score\", \"recall\"]:\n",
    "                print_log(f\"Train {key:<10}: {train_metric[key]}\", MODEL_FILE_DIRC)\n",
    "                print_log(f\"Valid {key:<10}: {valid_metric[key]}\", MODEL_FILE_DIRC)\n",
    "\n",
    "            \n",
    "            ## 3.1 Plot the loss function\n",
    "            fig,ax = plt.subplots(3,2, figsize=(10,10))\n",
    "            x_data = range(len(df[\"Train Loss\"]))\n",
    "            for i, key in enumerate([\"Loss\",\"precision\", \"accuracy\", \"f1_score\", \"recall\"]):    \n",
    "                ax[i%3][i//3].plot(x_data, df[f\"Train {key}\"], label=f\"Train {key}\")\n",
    "                ax[i%3][i//3].plot(x_data, df[f\"Valid {key}\"], label=f\"Valid {key}\")\n",
    "                ax[i%3][i//3].legend()\n",
    "            plt.savefig(f'{MODEL_FILE_DIRC}/Loss.png', transparent=False, facecolor='white')\n",
    "            plt.close('all')\n",
    "\n",
    "            ## 3.3. Save model and Stoping criteria\n",
    "            if prev_best_valid_f1 <= valid_metric[\"f1_score\"]:  # If previous best validation f1-score <= current f1-score \n",
    "                state_dict = {\n",
    "                    \"model\": model.state_dict(), \n",
    "                    \"epoch\":epoch,\n",
    "                    \"valid_f1_score\": valid_metric[\"f1_score\"],\n",
    "                    \"valid_loss\": valid_loss\n",
    "                }\n",
    "                torch.save(state_dict, f\"{MODEL_FILE_DIRC}/{model_name}_best.pt\")\n",
    "                prev_best_valid_f1 = valid_metric[\"f1_score\"]  # Previous validation loss = Current validation loss\n",
    "                count = 0\n",
    "            else:\n",
    "                count += 1\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                state_dict = {\n",
    "                    \"model\": model.state_dict(), \n",
    "                    \"epoch\":epoch,\n",
    "                    \"valid_f1_score\": valid_metric[\"f1_score\"],\n",
    "                    \"valid_loss\": valid_loss\n",
    "                }\n",
    "                torch.save(state_dict, f\"{MODEL_FILE_DIRC}/{model_name}_{epoch}.pt\")\n",
    "            \n",
    "            df.to_csv(f\"{MODEL_FILE_DIRC}/Loss.csv\", index=False)\n",
    "            \n",
    "            if count == MAX_COUNT_F1_SCORE:\n",
    "                print_log(f\"The validation f1 score is not increasing for continuous {MAX_COUNT_F1_SCORE} time, so training stop\", MODEL_FILE_DIRC)\n",
    "                break\n",
    "            \n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build and Train the SateLight model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build the SateLight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been loaded from the file 'SateLight_15.pt'\n",
      "The dataframe that record the loss have been loaded from Model/SateLight_synthesized/Loss.csv\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Sequential: 1-1                             [-1, 32, 1, 641]          --\n",
      "|    └─Conv2d: 2-1                            [-1, 16, 19, 641]         10,256\n",
      "|    └─Conv2d: 2-2                            [-1, 32, 1, 641]          640\n",
      "├─BatchNorm1d: 1-2                            [-1, 32, 641]             64\n",
      "├─ReLU: 1-3                                   [-1, 32, 641]             --\n",
      "├─Dropout: 1-4                                [-1, 32, 641]             --\n",
      "├─MaxPool1d: 1-5                              [-1, 32, 160]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-3                        [-1, 160, 32]             --\n",
      "|    |    └─SelfAttention: 3-1                [-1, 160, 32]             7,392\n",
      "|    |    └─BatchNorm1d: 3-2                  [-1, 160, 32]             320\n",
      "|    |    └─Dropout: 3-3                      [-1, 160, 32]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-4                        [-1, 160, 64]             --\n",
      "|    |    └─Linear: 3-4                       [-1, 160, 64]             2,112\n",
      "|    |    └─BatchNorm1d: 3-5                  [-1, 160, 64]             320\n",
      "|    |    └─ReLU: 3-6                         [-1, 160, 64]             --\n",
      "|    |    └─Dropout: 3-7                      [-1, 160, 64]             --\n",
      "├─MaxPool1d: 1-6                              [-1, 64, 40]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-5                        [-1, 40, 64]              --\n",
      "|    |    └─SelfAttention: 3-8                [-1, 40, 64]              29,120\n",
      "|    |    └─BatchNorm1d: 3-9                  [-1, 40, 64]              80\n",
      "|    |    └─Dropout: 3-10                     [-1, 40, 64]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-6                        [-1, 40, 96]              --\n",
      "|    |    └─Linear: 3-11                      [-1, 40, 96]              6,240\n",
      "|    |    └─BatchNorm1d: 3-12                 [-1, 40, 96]              80\n",
      "|    |    └─ReLU: 3-13                        [-1, 40, 96]              --\n",
      "|    |    └─Dropout: 3-14                     [-1, 40, 96]              --\n",
      "├─MaxPool1d: 1-7                              [-1, 96, 10]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-7                        [-1, 10, 96]              --\n",
      "|    |    └─SelfAttention: 3-15               [-1, 10, 96]              65,184\n",
      "|    |    └─BatchNorm1d: 3-16                 [-1, 10, 96]              20\n",
      "|    |    └─Dropout: 3-17                     [-1, 10, 96]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-8                        [-1, 10, 128]             --\n",
      "|    |    └─Linear: 3-18                      [-1, 10, 128]             12,416\n",
      "|    |    └─BatchNorm1d: 3-19                 [-1, 10, 128]             20\n",
      "|    |    └─ReLU: 3-20                        [-1, 10, 128]             --\n",
      "|    |    └─Dropout: 3-21                     [-1, 10, 128]             --\n",
      "├─MaxPool1d: 1-8                              [-1, 128, 2]              --\n",
      "├─Linear: 1-9                                 [-1, 1]                   257\n",
      "===============================================================================================\n",
      "Total params: 134,521\n",
      "Trainable params: 134,521\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 125.46\n",
      "===============================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 2.90\n",
      "===============================================================================================\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Sequential: 1-1                             [-1, 32, 1, 641]          --\n",
      "|    └─Conv2d: 2-1                            [-1, 16, 19, 641]         10,256\n",
      "|    └─Conv2d: 2-2                            [-1, 32, 1, 641]          640\n",
      "├─BatchNorm1d: 1-2                            [-1, 32, 641]             64\n",
      "├─ReLU: 1-3                                   [-1, 32, 641]             --\n",
      "├─Dropout: 1-4                                [-1, 32, 641]             --\n",
      "├─MaxPool1d: 1-5                              [-1, 32, 160]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-3                        [-1, 160, 32]             --\n",
      "|    |    └─SelfAttention: 3-1                [-1, 160, 32]             7,392\n",
      "|    |    └─BatchNorm1d: 3-2                  [-1, 160, 32]             320\n",
      "|    |    └─Dropout: 3-3                      [-1, 160, 32]             --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-4                        [-1, 160, 64]             --\n",
      "|    |    └─Linear: 3-4                       [-1, 160, 64]             2,112\n",
      "|    |    └─BatchNorm1d: 3-5                  [-1, 160, 64]             320\n",
      "|    |    └─ReLU: 3-6                         [-1, 160, 64]             --\n",
      "|    |    └─Dropout: 3-7                      [-1, 160, 64]             --\n",
      "├─MaxPool1d: 1-6                              [-1, 64, 40]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-5                        [-1, 40, 64]              --\n",
      "|    |    └─SelfAttention: 3-8                [-1, 40, 64]              29,120\n",
      "|    |    └─BatchNorm1d: 3-9                  [-1, 40, 64]              80\n",
      "|    |    └─Dropout: 3-10                     [-1, 40, 64]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-6                        [-1, 40, 96]              --\n",
      "|    |    └─Linear: 3-11                      [-1, 40, 96]              6,240\n",
      "|    |    └─BatchNorm1d: 3-12                 [-1, 40, 96]              80\n",
      "|    |    └─ReLU: 3-13                        [-1, 40, 96]              --\n",
      "|    |    └─Dropout: 3-14                     [-1, 40, 96]              --\n",
      "├─MaxPool1d: 1-7                              [-1, 96, 10]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-7                        [-1, 10, 96]              --\n",
      "|    |    └─SelfAttention: 3-15               [-1, 10, 96]              65,184\n",
      "|    |    └─BatchNorm1d: 3-16                 [-1, 10, 96]              20\n",
      "|    |    └─Dropout: 3-17                     [-1, 10, 96]              --\n",
      "├─ModuleList: 1                               []                        --\n",
      "|    └─Sequential: 2-8                        [-1, 10, 128]             --\n",
      "|    |    └─Linear: 3-18                      [-1, 10, 128]             12,416\n",
      "|    |    └─BatchNorm1d: 3-19                 [-1, 10, 128]             20\n",
      "|    |    └─ReLU: 3-20                        [-1, 10, 128]             --\n",
      "|    |    └─Dropout: 3-21                     [-1, 10, 128]             --\n",
      "├─MaxPool1d: 1-8                              [-1, 128, 2]              --\n",
      "├─Linear: 1-9                                 [-1, 1]                   257\n",
      "===============================================================================================\n",
      "Total params: 134,521\n",
      "Trainable params: 134,521\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 125.46\n",
      "===============================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 2.30\n",
      "Params size (MB): 0.51\n",
      "Estimated Total Size (MB): 2.90\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model      = SateLight().to(device)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Get the following information:\n",
    "# 1. Previous Trained model (if exist)\n",
    "# 2. df that store the training/validation loss & metrics\n",
    "# 3. epoch where the training start\n",
    "# 4. Previous Highest Validation Recall\n",
    "# 5. Previous Lowest  Validation Loss\n",
    "model, df, EPOCH_START, prev_best_valid_f1, prev_best_valid_loss = load_classification_model_dict(model, MODEL_FILE_DIRC_SateLight, \"SateLight\")\n",
    "\n",
    "# Load the previous train model from original data\n",
    "state_dict_loaded = torch.load(MODEL_FILE_DIRC + f\"/SateLight/SateLight_best.pt\")\n",
    "model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "\n",
    "# Get the summary of the model\n",
    "print(summary(model, (19,1280)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Print out the model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model infomation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Device used        : cuda\n",
      "BATCH SIZE         : 32\n",
      "MAX_COUNT_F1_SCORE : 10\n",
      "LEARNING RATE      : 0.0007\n",
      "Prev Best f1-score in validation dataset: 0.6666666666666666\n",
      "Prev Best validation loss             : 0.002288419742833114\n",
      "Number of EPOCH for training   : 101 (EPOCH start from 16)\n",
      "Num of epochs of data for train: 8345\n",
      "Num of epochs of data for valid: 1358\n",
      "Model parameters               : 134,521\n"
     ]
    }
   ],
   "source": [
    "seperate = \"\\n\" + \"-\" * 100 + \"\\n\"\n",
    "print(seperate + \"Model infomation\" + seperate)\n",
    "print(f\"Device used        :\", device)\n",
    "print(f\"BATCH SIZE         :\", BATCH_SIZE)\n",
    "print(f\"MAX_COUNT_F1_SCORE :\", MAX_COUNT_F1_SCORE)\n",
    "print(f\"LEARNING RATE      :\", LEARNING_RATE)\n",
    "print(f\"Prev Best f1-score in validation dataset:\", prev_best_valid_f1)\n",
    "print(f\"Prev Best validation loss             :\", prev_best_valid_loss)\n",
    "print(f\"Number of EPOCH for training   :\",NUM_EPOCHS_CLASSIFIER, f\"(EPOCH start from {EPOCH_START})\")\n",
    "print(f\"Num of epochs of data for train:\", num_train_data)\n",
    "print(f\"Num of epochs of data for valid:\", num_valid_data)\n",
    "print(f'Model parameters               : {sum(p.numel() for p in model.parameters()):,}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > Epoch     : 16\n",
      "Train loss      : 0.00027541496374544134\n",
      "Valid loss      : 0.0035445218616397024\n",
      "Train precision : 0.9985372988785958\n",
      "Valid precision : 0.2916666666666667\n",
      "Train accuracy  : 0.9994008388256441\n",
      "Valid accuracy  : 0.9845360824742269\n",
      "Train f1_score  : 0.9987807851743478\n",
      "Valid f1_score  : 0.4\n",
      "Train recall    : 0.9990243902439024\n",
      "Valid recall    : 0.6363636363636364\n",
      "> > > Epoch     : 17\n",
      "Train loss      : 0.00031040236297289885\n",
      "Valid loss      : 0.0033065392878580363\n",
      "Train precision : 0.9975633528265108\n",
      "Valid precision : 0.3888888888888889\n",
      "Train accuracy  : 0.9990413421210306\n",
      "Valid accuracy  : 0.9889543446244478\n",
      "Train f1_score  : 0.9980497318381277\n",
      "Valid f1_score  : 0.4827586206896552\n",
      "Train recall    : 0.9985365853658537\n",
      "Valid recall    : 0.6363636363636364\n",
      "> > > Epoch     : 18\n",
      "Train loss      : 0.0006346530464765653\n",
      "Valid loss      : 0.004027733379816026\n",
      "Train precision : 0.99609375\n",
      "Valid precision : 0.6666666666666666\n",
      "Train accuracy  : 0.9978430197723187\n",
      "Valid accuracy  : 0.9926362297496318\n",
      "Train f1_score  : 0.9956076134699854\n",
      "Valid f1_score  : 0.2857142857142857\n",
      "Train recall    : 0.9951219512195122\n",
      "Valid recall    : 0.18181818181818182\n",
      "> > > Epoch     : 19\n",
      "Train loss      : 0.0005242239483950235\n",
      "Valid loss      : 0.0025857235649628055\n",
      "Train precision : 0.9970703125\n",
      "Valid precision : 0.5714285714285714\n",
      "Train accuracy  : 0.9983223487118035\n",
      "Valid accuracy  : 0.9926362297496318\n",
      "Train f1_score  : 0.9965836993655441\n",
      "Valid f1_score  : 0.4444444444444444\n",
      "Train recall    : 0.9960975609756098\n",
      "Valid recall    : 0.36363636363636365\n",
      "> > > Epoch     : 20\n",
      "Train loss      : 0.0006065664212597682\n",
      "Valid loss      : 0.0025839278812123125\n",
      "Train precision : 0.9970688812896922\n",
      "Valid precision : 0.8333333333333334\n",
      "Train accuracy  : 0.9982025164769323\n",
      "Valid accuracy  : 0.9948453608247423\n",
      "Train f1_score  : 0.9963387844764462\n",
      "Valid f1_score  : 0.5882352941176471\n",
      "Train recall    : 0.995609756097561\n",
      "Valid recall    : 0.45454545454545453\n",
      "> > > Epoch     : 21\n",
      "Train loss      : 0.0003494892883507262\n",
      "Valid loss      : 0.0032843550610341375\n",
      "Train precision : 0.9985344406448461\n",
      "Valid precision : 0.6666666666666666\n",
      "Train accuracy  : 0.9989215098861594\n",
      "Valid accuracy  : 0.9926362297496318\n",
      "Train f1_score  : 0.9978032706858677\n",
      "Valid f1_score  : 0.2857142857142857\n",
      "Train recall    : 0.9970731707317073\n",
      "Valid recall    : 0.18181818181818182\n",
      "> > > Epoch     : 22\n",
      "Train loss      : 0.00024888897827101417\n",
      "Valid loss      : 0.0034473868448958523\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.5714285714285714\n",
      "Train accuracy  : 0.9996405032953865\n",
      "Valid accuracy  : 0.9926362297496318\n",
      "Train f1_score  : 0.9992677568952892\n",
      "Valid f1_score  : 0.4444444444444444\n",
      "Train recall    : 0.9985365853658537\n",
      "Valid recall    : 0.36363636363636365\n",
      "> > > Epoch     : 23\n",
      "Train loss      : 0.00048042404008465605\n",
      "Valid loss      : 0.0029579438858930245\n",
      "Train precision : 0.9990210474791973\n",
      "Valid precision : 0.375\n",
      "Train accuracy  : 0.998681845416417\n",
      "Valid accuracy  : 0.9889543446244478\n",
      "Train f1_score  : 0.9973124847300269\n",
      "Valid f1_score  : 0.4444444444444444\n",
      "Train recall    : 0.995609756097561\n",
      "Valid recall    : 0.5454545454545454\n",
      "> > > Epoch     : 24\n",
      "Train loss      : 6.586872022094964e-05\n",
      "Valid loss      : 0.0031536520208739257\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.36363636363636365\n",
      "Train accuracy  : 0.9998801677651288\n",
      "Valid accuracy  : 0.9896907216494846\n",
      "Train f1_score  : 0.9997560380580629\n",
      "Valid f1_score  : 0.36363636363636365\n",
      "Train recall    : 0.9995121951219512\n",
      "Valid recall    : 0.36363636363636365\n",
      "> > > Epoch     : 25\n",
      "Train loss      : 0.00015405960071170684\n",
      "Valid loss      : 0.00217423581277494\n",
      "Train precision : 0.9995119570522206\n",
      "Valid precision : 0.7142857142857143\n",
      "Train accuracy  : 0.9996405032953865\n",
      "Valid accuracy  : 0.9941089837997055\n",
      "Train f1_score  : 0.9992681141741888\n",
      "Valid f1_score  : 0.5555555555555556\n",
      "Train recall    : 0.9990243902439024\n",
      "Valid recall    : 0.45454545454545453\n",
      "The validation f1 score is not increasing for continuous 10 time, so training stop\n",
      "CPU times: total: 4min 22s\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_classification_model_training(EPOCH_START, NUM_EPOCHS_CLASSIFIER, \n",
    "                                    model, MODEL_FILE_DIRC_SateLight, \"SateLight\",\n",
    "                                    df, prev_best_valid_f1, prev_best_valid_loss, \n",
    "                                    train_data, num_train_data, \n",
    "                                    valid_data, num_valid_data, \n",
    "                                    scheduler, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Test the model performance on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is at epoch: 8\n",
      "Metric on testing dataset:\n",
      "precision : 0.7143\n",
      "accuracy  : 0.9941\n",
      "f1_score  : 0.5556\n",
      "recall    : 0.4545\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and turn to evaluation mode\n",
    "model.eval()\n",
    "state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC_SateLight}/SateLight_best.pt\")\n",
    "model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "\n",
    "test_loss, test_metric = evaluate_classifier(model, test_data, device, num_test_data, LOSS_POS_WEIGHT=torch.tensor([3])) \n",
    "\n",
    "print(\"Best model is at epoch:\",state_dict_loaded[\"epoch\"])\n",
    "print(\"Metric on testing dataset:\")\n",
    "for key, value in test_metric.items():\n",
    "    print(f\"{key:<10}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Simple CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 32, 19]              --\n",
      "|    └─Conv1dWithInitialization: 2-1     [-1, 8, 1278]             --\n",
      "|    |    └─Conv1d: 3-1                  [-1, 8, 1278]             464\n",
      "|    └─BatchNorm1d: 2-2                  [-1, 8, 1278]             16\n",
      "|    └─ReLU: 2-3                         [-1, 8, 1278]             --\n",
      "|    └─MaxPool1d: 2-4                    [-1, 8, 319]              --\n",
      "|    └─Conv1dWithInitialization: 2-5     [-1, 16, 317]             --\n",
      "|    |    └─Conv1d: 3-2                  [-1, 16, 317]             400\n",
      "|    └─BatchNorm1d: 2-6                  [-1, 16, 317]             32\n",
      "|    └─ReLU: 2-7                         [-1, 16, 317]             --\n",
      "|    └─MaxPool1d: 2-8                    [-1, 16, 79]              --\n",
      "|    └─Conv1dWithInitialization: 2-9     [-1, 32, 77]              --\n",
      "|    |    └─Conv1d: 3-3                  [-1, 32, 77]              1,568\n",
      "|    └─BatchNorm1d: 2-10                 [-1, 32, 77]              64\n",
      "|    └─ReLU: 2-11                        [-1, 32, 77]              --\n",
      "|    └─MaxPool1d: 2-12                   [-1, 32, 19]              --\n",
      "├─Linear: 1-2                            [-1, 1]                   609\n",
      "==========================================================================================\n",
      "Total params: 3,153\n",
      "Trainable params: 3,153\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.83\n",
      "==========================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.27\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.38\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 32, 19]              --\n",
      "|    └─Conv1dWithInitialization: 2-1     [-1, 8, 1278]             --\n",
      "|    |    └─Conv1d: 3-1                  [-1, 8, 1278]             464\n",
      "|    └─BatchNorm1d: 2-2                  [-1, 8, 1278]             16\n",
      "|    └─ReLU: 2-3                         [-1, 8, 1278]             --\n",
      "|    └─MaxPool1d: 2-4                    [-1, 8, 319]              --\n",
      "|    └─Conv1dWithInitialization: 2-5     [-1, 16, 317]             --\n",
      "|    |    └─Conv1d: 3-2                  [-1, 16, 317]             400\n",
      "|    └─BatchNorm1d: 2-6                  [-1, 16, 317]             32\n",
      "|    └─ReLU: 2-7                         [-1, 16, 317]             --\n",
      "|    └─MaxPool1d: 2-8                    [-1, 16, 79]              --\n",
      "|    └─Conv1dWithInitialization: 2-9     [-1, 32, 77]              --\n",
      "|    |    └─Conv1d: 3-3                  [-1, 32, 77]              1,568\n",
      "|    └─BatchNorm1d: 2-10                 [-1, 32, 77]              64\n",
      "|    └─ReLU: 2-11                        [-1, 32, 77]              --\n",
      "|    └─MaxPool1d: 2-12                   [-1, 32, 19]              --\n",
      "├─Linear: 1-2                            [-1, 1]                   609\n",
      "==========================================================================================\n",
      "Total params: 3,153\n",
      "Trainable params: 3,153\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.83\n",
      "==========================================================================================\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 0.27\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.38\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model      = CNN().to(device)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Get the following information:\n",
    "# 1. Previous Trained model (if exist)\n",
    "# 2. df that store the training/validation loss & metrics\n",
    "# 3. epoch where the training start\n",
    "# 4. Previous Highest Validation Recall\n",
    "# 5. Previous Lowest  Validation Loss\n",
    "model, df, EPOCH_START, prev_best_valid_f1, prev_best_valid_loss = load_classification_model_dict(model, MODEL_FILE_DIRC_CNN, \"CNN\")\n",
    "\n",
    "# Load the previous train model from original data\n",
    "state_dict_loaded = torch.load(MODEL_FILE_DIRC + f\"/CNN/CNN_best.pt\")\n",
    "model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "\n",
    "# Get the summary of the model\n",
    "print(summary(model, (19,1280)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model infomation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Device used        : cuda\n",
      "BATCH SIZE         : 32\n",
      "MAX_COUNT_F1_SCORE : 10\n",
      "LEARNING RATE      : 0.0007\n",
      "Prev Best recall in validation dataset: -1\n",
      "Prev Best validation loss             : 10000\n",
      "Number of EPOCH for training   : 101 (EPOCH start from 1)\n",
      "Num of epochs of data for train: 8345\n",
      "Num of epochs of data for valid: 1358\n",
      "Model parameters               : 3,153\n"
     ]
    }
   ],
   "source": [
    "seperate = \"\\n\" + \"-\" * 100 + \"\\n\"\n",
    "print(seperate + \"Model infomation\" + seperate)\n",
    "print(f\"Device used        :\", device)\n",
    "print(f\"BATCH SIZE         :\", BATCH_SIZE)\n",
    "print(f\"MAX_COUNT_F1_SCORE :\", MAX_COUNT_F1_SCORE)\n",
    "print(f\"LEARNING RATE      :\", LEARNING_RATE)\n",
    "print(f\"Prev Best recall in validation dataset:\", prev_best_valid_f1)\n",
    "print(f\"Prev Best validation loss             :\", prev_best_valid_loss)\n",
    "print(f\"Number of EPOCH for training   :\",NUM_EPOCHS_CLASSIFIER, f\"(EPOCH start from {EPOCH_START})\")\n",
    "print(f\"Num of epochs of data for train:\", num_train_data)\n",
    "print(f\"Num of epochs of data for valid:\", num_valid_data)\n",
    "print(f'Model parameters               : {sum(p.numel() for p in model.parameters()):,}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> > > Epoch     : 1\n",
      "Train loss      : 0.015580812288259048\n",
      "Valid loss      : 0.004591986099436374\n",
      "Train precision : 0.9811031664964249\n",
      "Valid precision : 0.25\n",
      "Train accuracy  : 0.980107849011384\n",
      "Valid accuracy  : 0.9904270986745214\n",
      "Train f1_score  : 0.9585828343313373\n",
      "Valid f1_score  : 0.13333333333333333\n",
      "Train recall    : 0.9370731707317074\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 2\n",
      "Train loss      : 0.0003509270016049856\n",
      "Valid loss      : 0.003967889056252816\n",
      "Train precision : 0.995609756097561\n",
      "Valid precision : 0.25\n",
      "Train accuracy  : 0.9978430197723187\n",
      "Valid accuracy  : 0.9904270986745214\n",
      "Train f1_score  : 0.995609756097561\n",
      "Valid f1_score  : 0.13333333333333333\n",
      "Train recall    : 0.995609756097561\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 3\n",
      "Train loss      : 0.00029916116441840146\n",
      "Valid loss      : 0.0031605228568757824\n",
      "Train precision : 0.9985322896281801\n",
      "Valid precision : 0.3333333333333333\n",
      "Train accuracy  : 0.9985620131815458\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 0.9970688812896922\n",
      "Valid f1_score  : 0.14285714285714285\n",
      "Train recall    : 0.995609756097561\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 4\n",
      "Train loss      : 0.0003792811411904503\n",
      "Valid loss      : 0.003850063574110116\n",
      "Train precision : 0.9960918417195896\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 0.9977231875374476\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 0.9953624603368318\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 0.9946341463414634\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 5\n",
      "Train loss      : 0.00020763999546698849\n",
      "Valid loss      : 0.003330202704632932\n",
      "Train precision : 0.9980459208597948\n",
      "Valid precision : 0.4\n",
      "Train accuracy  : 0.998681845416417\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 0.9973151086160605\n",
      "Valid f1_score  : 0.25\n",
      "Train recall    : 0.9965853658536585\n",
      "Valid recall    : 0.18181818181818182\n",
      "> > > Epoch     : 6\n",
      "Train loss      : 0.00017027383335597437\n",
      "Valid loss      : 0.004826456709303351\n",
      "Train precision : 0.9980487804878049\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 0.9990413421210306\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 0.9980487804878049\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 0.9980487804878049\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 7\n",
      "Train loss      : 6.758431693272363e-05\n",
      "Valid loss      : 0.004737807040157888\n",
      "Train precision : 0.9990253411306043\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 0.9997603355302577\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 0.999512432959532\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 8\n",
      "Train loss      : 0.00010692201412774179\n",
      "Valid loss      : 0.003715130248708724\n",
      "Train precision : 0.9990248659190639\n",
      "Valid precision : 0.25\n",
      "Train accuracy  : 0.9996405032953865\n",
      "Valid accuracy  : 0.9904270986745214\n",
      "Train f1_score  : 0.9992684711046086\n",
      "Valid f1_score  : 0.13333333333333333\n",
      "Train recall    : 0.9995121951219512\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 9\n",
      "Train loss      : 0.00011450726512015515\n",
      "Valid loss      : 0.003658034075917803\n",
      "Train precision : 0.9980497318381277\n",
      "Valid precision : 0.3\n",
      "Train accuracy  : 0.9991611743559017\n",
      "Valid accuracy  : 0.9889543446244478\n",
      "Train f1_score  : 0.9982930992440868\n",
      "Valid f1_score  : 0.2857142857142857\n",
      "Train recall    : 0.9985365853658537\n",
      "Valid recall    : 0.2727272727272727\n",
      "> > > Epoch     : 10\n",
      "Train loss      : 0.00013172276031577537\n",
      "Valid loss      : 0.004022048344817294\n",
      "Train precision : 0.997078870496592\n",
      "Valid precision : 0.3333333333333333\n",
      "Train accuracy  : 0.9990413421210306\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 0.9980506822612085\n",
      "Valid f1_score  : 0.14285714285714285\n",
      "Train recall    : 0.9990243902439024\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 11\n",
      "Train loss      : 9.192148019101536e-05\n",
      "Valid loss      : 0.0036741056922093324\n",
      "Train precision : 0.9985372988785958\n",
      "Valid precision : 0.3333333333333333\n",
      "Train accuracy  : 0.9994008388256441\n",
      "Valid accuracy  : 0.9904270986745214\n",
      "Train f1_score  : 0.9987807851743478\n",
      "Valid f1_score  : 0.23529411764705882\n",
      "Train recall    : 0.9990243902439024\n",
      "Valid recall    : 0.18181818181818182\n",
      "> > > Epoch     : 12\n",
      "Train loss      : 7.687265860412426e-05\n",
      "Valid loss      : 0.0034376323124186707\n",
      "Train precision : 0.9995121951219512\n",
      "Valid precision : 0.42857142857142855\n",
      "Train accuracy  : 0.9997603355302577\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 0.9995121951219512\n",
      "Valid f1_score  : 0.3333333333333333\n",
      "Train recall    : 0.9995121951219512\n",
      "Valid recall    : 0.2727272727272727\n",
      "> > > Epoch     : 13\n",
      "Train loss      : 5.698575946741321e-05\n",
      "Valid loss      : 0.0038324818823597838\n",
      "Train precision : 0.9990248659190639\n",
      "Valid precision : 0.42857142857142855\n",
      "Train accuracy  : 0.9996405032953865\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 0.9992684711046086\n",
      "Valid f1_score  : 0.3333333333333333\n",
      "Train recall    : 0.9995121951219512\n",
      "Valid recall    : 0.2727272727272727\n",
      "> > > Epoch     : 14\n",
      "Train loss      : 3.628214588458552e-05\n",
      "Valid loss      : 0.003738332712833889\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.42857142857142855\n",
      "Train accuracy  : 0.9998801677651288\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 0.9997560380580629\n",
      "Valid f1_score  : 0.3333333333333333\n",
      "Train recall    : 0.9995121951219512\n",
      "Valid recall    : 0.2727272727272727\n",
      "> > > Epoch     : 15\n",
      "Train loss      : 3.2094713041723195e-05\n",
      "Valid loss      : 0.003926888794834914\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.3333333333333333\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.14285714285714285\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 16\n",
      "Train loss      : 2.748645097637859e-05\n",
      "Valid loss      : 0.004584059395457779\n",
      "Train precision : 0.999512432959532\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 0.9998801677651288\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 0.9997561570348695\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 17\n",
      "Train loss      : 0.00019810651685536054\n",
      "Valid loss      : 0.004633881242006482\n",
      "Train precision : 0.9946627850557982\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 0.998681845416417\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 0.997324252006811\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 18\n",
      "Train loss      : 2.5216308918968575e-05\n",
      "Valid loss      : 0.004459165960443309\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.25\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9904270986745214\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.13333333333333333\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 19\n",
      "Train loss      : 2.341126606966974e-05\n",
      "Valid loss      : 0.0036226436816381175\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.26666666666666666\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.18181818181818182\n",
      "> > > Epoch     : 20\n",
      "Train loss      : 1.4183233999499239e-05\n",
      "Valid loss      : 0.003941795239669919\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 21\n",
      "Train loss      : 4.989331651092207e-05\n",
      "Valid loss      : 0.004038679300103953\n",
      "Train precision : 0.9985380116959064\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 0.9995206710605152\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 0.9990248659190639\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 0.9995121951219512\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 22\n",
      "Train loss      : 3.870340995669289e-05\n",
      "Valid loss      : 0.003650207784425828\n",
      "Train precision : 0.999512432959532\n",
      "Valid precision : 0.3333333333333333\n",
      "Train accuracy  : 0.9998801677651288\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 0.9997561570348695\n",
      "Valid f1_score  : 0.14285714285714285\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 23\n",
      "Train loss      : 1.145454882149513e-05\n",
      "Valid loss      : 0.004192099028259771\n",
      "Train precision : 1.0\n",
      "Valid precision : 0.3333333333333333\n",
      "Train accuracy  : 1.0\n",
      "Valid accuracy  : 0.9911634756995582\n",
      "Train f1_score  : 1.0\n",
      "Valid f1_score  : 0.14285714285714285\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "> > > Epoch     : 24\n",
      "Train loss      : 2.2808256620147938e-05\n",
      "Valid loss      : 0.004182015767716871\n",
      "Train precision : 0.999512432959532\n",
      "Valid precision : 0.5\n",
      "Train accuracy  : 0.9998801677651288\n",
      "Valid accuracy  : 0.991899852724595\n",
      "Train f1_score  : 0.9997561570348695\n",
      "Valid f1_score  : 0.15384615384615385\n",
      "Train recall    : 1.0\n",
      "Valid recall    : 0.09090909090909091\n",
      "The validation f1 score is not increasing for continuous 10 time, so training stop\n",
      "CPU times: total: 1min 10s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_classification_model_training(EPOCH_START, NUM_EPOCHS_CLASSIFIER, \n",
    "                                    model, MODEL_FILE_DIRC_CNN, \"CNN\",\n",
    "                                    df, prev_best_valid_f1, prev_best_valid_loss, \n",
    "                                    train_data, num_train_data, \n",
    "                                    valid_data, num_valid_data, \n",
    "                                    scheduler, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model performance on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is at epoch: 14\n",
      "Metric on testing dataset:\n",
      "precision : 0.6000\n",
      "accuracy  : 0.9926\n",
      "f1_score  : 0.3750\n",
      "recall    : 0.2727\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and turn to evaluation mode\n",
    "model.eval()\n",
    "state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC_CNN}/CNN_best.pt\")\n",
    "model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "\n",
    "test_loss, test_metric = evaluate_classifier(model, test_data, device, num_test_data, LOSS_POS_WEIGHT=torch.tensor([3]))\n",
    "\n",
    "print(\"Best model is at epoch:\",state_dict_loaded[\"epoch\"])\n",
    "print(\"Metric on testing dataset:\")\n",
    "for key, value in test_metric.items():\n",
    "    print(f\"{key:<10}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
