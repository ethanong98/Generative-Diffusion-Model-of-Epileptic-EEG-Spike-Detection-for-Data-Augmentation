{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12f01b05cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mne\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt \n",
    "from PIL import Image\n",
    "from utility import *\n",
    "from model import *\n",
    "\n",
    "MODEL_FILE_DIRC_WaveGrad = MODEL_FILE_DIRC + \"/WaveGrad\"\n",
    "os.makedirs(MODEL_FILE_DIRC_WaveGrad, exist_ok=True)\n",
    "\n",
    "torch.manual_seed(3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "* Get the train, validation, test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data from EEG_csv/eeg1.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(252, 1280, 19)\n",
      "EEG1 has 252 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg2.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(43, 1280, 19)\n",
      "EEG2 has 43 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg3.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(88, 1280, 19)\n",
      "EEG3 has 88 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg4.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(245, 1280, 19)\n",
      "EEG4 has 245 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg5.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(236, 1280, 19)\n",
      "EEG5 has 236 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg6.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (481920, 19)\n",
      "Data with spike   : (465280, 19)\n",
      "Data without spike: (16640, 19)\n",
      "Data after  split into window: (376, 1280, 19)\n",
      "Labels: (376,)\n",
      "Num spike: 13\n",
      "EEG6 has 376 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg7.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(311, 1280, 19)\n",
      "EEG7 has 311 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg8.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(243, 1280, 19)\n",
      "EEG8 has 243 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg9.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(322, 1280, 19)\n",
      "EEG9 has 322 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg10.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(422, 1280, 19)\n",
      "EEG10 has 422 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg11.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(126, 1280, 19)\n",
      "EEG11 has 126 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg12.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(48, 1280, 19)\n",
      "EEG12 has 48 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg13.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(182, 1280, 19)\n",
      "EEG13 has 182 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg14.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(217, 1280, 19)\n",
      "EEG14 has 217 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg15.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(319, 1280, 19)\n",
      "EEG15 has 319 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg16.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(58, 1280, 19)\n",
      "EEG16 has 58 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg17.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(208, 1280, 19)\n",
      "EEG17 has 208 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg18.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(313, 1280, 19)\n",
      "EEG18 has 313 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg19.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (413568, 19)\n",
      "Data with spike   : (411008, 19)\n",
      "Data without spike: (2560, 19)\n",
      "Data after  split into window: (323, 1280, 19)\n",
      "Labels: (323,)\n",
      "Num spike: 2\n",
      "EEG19 has 323 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg20.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(255, 1280, 19)\n",
      "EEG20 has 255 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg21.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(363, 1280, 19)\n",
      "EEG21 has 363 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg22.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(15, 1280, 19)\n",
      "EEG22 has 15 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg23.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(308, 1280, 19)\n",
      "EEG23 has 308 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg24.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (386560, 19)\n",
      "Data with spike   : (385280, 19)\n",
      "Data without spike: (1280, 19)\n",
      "Data after  split into window: (302, 1280, 19)\n",
      "Labels: (302,)\n",
      "Num spike: 1\n",
      "EEG24 has 302 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg25.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(426, 1280, 19)\n",
      "EEG25 has 426 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg26.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(237, 1280, 19)\n",
      "EEG26 has 237 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg27.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(147, 1280, 19)\n",
      "EEG27 has 147 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg28.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(173, 1280, 19)\n",
      "EEG28 has 173 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg29.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(29, 1280, 19)\n",
      "EEG29 has 29 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg30.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(300, 1280, 19)\n",
      "EEG30 has 300 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg31.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (381312, 19)\n",
      "Data with spike   : (341632, 19)\n",
      "Data without spike: (39680, 19)\n",
      "Data after  split into window: (297, 1280, 19)\n",
      "Labels: (297,)\n",
      "Num spike: 31\n",
      "EEG31 has 297 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg32.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(254, 1280, 19)\n",
      "EEG32 has 254 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg33.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(65, 1280, 19)\n",
      "EEG33 has 65 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg34.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(264, 1280, 19)\n",
      "EEG34 has 264 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg35.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(297, 1280, 19)\n",
      "EEG35 has 297 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg36.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (344448, 19)\n",
      "Data with spike   : (313728, 19)\n",
      "Data without spike: (30720, 19)\n",
      "Data after  split into window: (269, 1280, 19)\n",
      "Labels: (269,)\n",
      "Num spike: 24\n",
      "EEG36 has 269 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg37.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(300, 1280, 19)\n",
      "EEG37 has 300 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg38.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(94, 1280, 19)\n",
      "EEG38 has 94 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg39.csv is loaded \n",
      "There is spike in this eeg file\n",
      "Data before split : (55296, 19)\n",
      "Data with spike   : (54016, 19)\n",
      "Data without spike: (1280, 19)\n",
      "Data after  split into window: (43, 1280, 19)\n",
      "Labels: (43,)\n",
      "Num spike: 1\n",
      "EEG39 has 43 windows of data \n",
      "\n",
      "\n",
      "The data from EEG_csv/eeg40.csv is loaded \n",
      "There is no spike in this eeg file\n",
      "(291, 1280, 19)\n",
      "EEG40 has 291 windows of data \n",
      "\n",
      "\n",
      "> > > Train    data  has shape: torch.Size([6345, 19, 1280]) when duration = 10 seconds\n",
      "> > > Data after DWT has shape: torch.Size([6345, 19, 1282])\n",
      "> > > Label              shape: torch.Size([6345])\n",
      "CPU times: total: 3min 16s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataloaders, num_data = get_dataloader(list(range(1,41)), shuffle=True)\n",
    "    \n",
    "num_train_data, num_valid_data, num_test_data = num_data\n",
    "train_data, valid_data, test_data             = dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model, EMA model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been loaded from the file 'Advanced_Diffusion_190.pt'\n",
      "The dataframe that record the loss have been loaded from {MODEL_FILE_DIRC_WaveGrad}/Loss.csv\n"
     ]
    }
   ],
   "source": [
    "model     = WaveGradNN(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "ema       = EMA(0.9) \n",
    "ema.register(model)\n",
    "\n",
    "\n",
    "list_model = os.listdir(MODEL_FILE_DIRC_WaveGrad)\n",
    "if len(list_model) > 0:    # Load the latest trained model\n",
    "    if os.path.exists(f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_best.pt\"):\n",
    "        state_dict_loaded    = torch.load(f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_best.pt\")\n",
    "        prev_best_valid_loss = state_dict_loaded[\"valid_loss\"]\n",
    "    list_model.remove(\"Advanced_Diffusion_best.pt\")\n",
    "    num_list   = [int(model_dir[model_dir.rindex(\"_\") +1: model_dir.rindex(\".\")]) for model_dir in list_model if model_dir.endswith(\".pt\")]\n",
    "    num_max    = np.max(num_list)\n",
    "    \n",
    "    state_dict_loaded = torch.load(f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_{num_max}.pt\")\n",
    "    model.load_state_dict(state_dict_loaded[\"model\"])\n",
    "    ema.load_state_dict(state_dict_loaded[\"model\"])\n",
    "    EPOCH_START = state_dict_loaded[\"epoch\"] + 1\n",
    "    \n",
    "    print(f\"The model has been loaded from the file 'Advanced_Diffusion_{num_max}.pt'\")\n",
    "\n",
    "    if os.path.exists(f\"{MODEL_FILE_DIRC_WaveGrad}/Loss.csv\"):\n",
    "        df = pd.read_csv(f\"{MODEL_FILE_DIRC_WaveGrad}/Loss.csv\")\n",
    "        df = df.iloc[:EPOCH_START-1, :]\n",
    "        print(f\"The dataframe that record the loss have been loaded from {{MODEL_FILE_DIRC_WaveGrad}}/Loss.csv\")\n",
    "\n",
    "else:\n",
    "    EPOCH_START          = 1\n",
    "    prev_best_valid_loss = 10000 \n",
    "    df                   = pd.DataFrame(columns = [\"Train Loss\", \"Valid Loss\"])\n",
    "\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model infomation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Device used    : cuda\n",
      "BATCH SIZE     : 32\n",
      "MAX_COUNT      : 30\n",
      "LEARNING RATE  : 0.001\n",
      "Previous best validation loss  : 0.002994722197043527\n",
      "Number of EPOCH for training   : 10000 (EPOCH start from 191)\n",
      "Num of epochs of data for train: 6345\n",
      "Num of epochs of data for valid: 1358\n",
      "Model parameters               : 34,342,899\n"
     ]
    }
   ],
   "source": [
    "seperate = \"\\n\" + \"-\" * 100 + \"\\n\"\n",
    "print(seperate + \"Model infomation\" + seperate)\n",
    "print(f\"Device used    :\", device)\n",
    "print(f\"BATCH SIZE     :\", BATCH_SIZE)\n",
    "print(f\"MAX_COUNT      :\", MAX_COUNT)\n",
    "print(f\"LEARNING RATE  :\", LEARNING_RATE)\n",
    "print(f\"Previous best validation loss  :\", prev_best_valid_loss)\n",
    "print(f\"Number of EPOCH for training   :\",NUM_EPOCHS, f\"(EPOCH start from {EPOCH_START})\")\n",
    "print(f\"Num of epochs of data for train:\", num_train_data)\n",
    "print(f\"Num of epochs of data for valid:\", num_valid_data)\n",
    "print(f'Model parameters               : {sum(p.numel() for p in model.parameters()):,}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(string_to_print):\n",
    "    with open(f'{MODEL_FILE_DIRC_WaveGrad}/log.txt', \"a\") as f:\n",
    "        print(string_to_print, file=f)\n",
    "    print(string_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     : 191\n",
      "Train loss: 0.003143619748318261\n",
      "Valid loss: 0.003069988469729016\n",
      "Epoch     : 192\n",
      "Train loss: 0.002970780183871587\n",
      "Valid loss: 0.0032528714497679287\n",
      "Epoch     : 193\n",
      "Train loss: 0.0028964651597354054\n",
      "Valid loss: 0.0032343153892543074\n",
      "Epoch     : 194\n",
      "Train loss: 0.003016096330045996\n",
      "Valid loss: 0.0032673061541411944\n",
      "Epoch     : 195\n",
      "Train loss: 0.002951352814927263\n",
      "Valid loss: 0.003210989437415779\n",
      "Epoch     : 196\n",
      "Train loss: 0.0029966896166474543\n",
      "Valid loss: 0.0030261687056568833\n",
      "Epoch     : 197\n",
      "Train loss: 0.0029282559246599817\n",
      "Valid loss: 0.003254647036144765\n",
      "Epoch     : 198\n",
      "Train loss: 0.002900797966500734\n",
      "Valid loss: 0.0034662412098212516\n",
      "Epoch     : 199\n",
      "Train loss: 0.002969954384181141\n",
      "Valid loss: 0.0030952761435280494\n",
      "Epoch     : 200\n",
      "Train loss: 0.00290882607453728\n",
      "Valid loss: 0.003057828282770071\n",
      "Epoch     : 201\n",
      "Train loss: 0.0029125687642018562\n",
      "Valid loss: 0.0033756129180296532\n",
      "Epoch     : 202\n",
      "Train loss: 0.00296242395379207\n",
      "Valid loss: 0.0030977007424094014\n",
      "Epoch     : 203\n",
      "Train loss: 0.002968277289606249\n",
      "Valid loss: 0.003255038608595268\n",
      "Epoch     : 204\n",
      "Train loss: 0.002908789485669493\n",
      "Valid loss: 0.003121208934461772\n",
      "Epoch     : 205\n",
      "Train loss: 0.00292427141932731\n",
      "Valid loss: 0.0031264060573304\n",
      "Epoch     : 206\n",
      "Train loss: 0.002915884282548565\n",
      "Valid loss: 0.002968494024605042\n",
      "Epoch     : 207\n",
      "Train loss: 0.0028776103529383388\n",
      "Valid loss: 0.003185243166116271\n",
      "Epoch     : 208\n",
      "Train loss: 0.002888346542116222\n",
      "Valid loss: 0.003190531871324084\n",
      "Epoch     : 209\n",
      "Train loss: 0.0029109421838588653\n",
      "Valid loss: 0.0030459900755594334\n",
      "Epoch     : 210\n",
      "Train loss: 0.0029203349634875832\n",
      "Valid loss: 0.003079279881404145\n",
      "Epoch     : 211\n",
      "Train loss: 0.002864354788975231\n",
      "Valid loss: 0.0031065760792091776\n",
      "Epoch     : 212\n",
      "Train loss: 0.002963905598912491\n",
      "Valid loss: 0.0029732091475788377\n",
      "Epoch     : 213\n",
      "Train loss: 0.0028746616180437205\n",
      "Valid loss: 0.0031202236774743744\n",
      "Epoch     : 214\n",
      "Train loss: 0.0027870021419932845\n",
      "Valid loss: 0.003131647908371573\n",
      "Epoch     : 215\n",
      "Train loss: 0.0028599684918556184\n",
      "Valid loss: 0.003052698335828416\n",
      "Epoch     : 216\n",
      "Train loss: 0.0028797077160355048\n",
      "Valid loss: 0.0031237994678680607\n",
      "Epoch     : 217\n",
      "Train loss: 0.0028783801170998054\n",
      "Valid loss: 0.0032601854949092304\n",
      "Epoch     : 218\n",
      "Train loss: 0.0029154475205897157\n",
      "Valid loss: 0.00287620279943469\n",
      "Epoch     : 219\n",
      "Train loss: 0.002893472779351393\n",
      "Valid loss: 0.0031202584230759357\n",
      "Epoch     : 220\n",
      "Train loss: 0.0029316736386694056\n",
      "Valid loss: 0.0030325969491861705\n",
      "Epoch     : 221\n",
      "Train loss: 0.002893021593561691\n",
      "Valid loss: 0.003215912713042415\n",
      "Epoch     : 222\n",
      "Train loss: 0.0028327034074457917\n",
      "Valid loss: 0.0032452248333128984\n",
      "Epoch     : 223\n",
      "Train loss: 0.002827722351437389\n",
      "Valid loss: 0.003035034084574639\n",
      "Epoch     : 224\n",
      "Train loss: 0.002892031756041086\n",
      "Valid loss: 0.0029919345908426573\n",
      "Epoch     : 225\n",
      "Train loss: 0.0028830801100830474\n",
      "Valid loss: 0.0029239117089708356\n",
      "Epoch     : 226\n",
      "Train loss: 0.0028516228505248062\n",
      "Valid loss: 0.003081792732357803\n",
      "Epoch     : 227\n",
      "Train loss: 0.002838325035487506\n",
      "Valid loss: 0.003086531098619301\n",
      "Epoch     : 228\n",
      "Train loss: 0.00283192975007708\n",
      "Valid loss: 0.0029832061122285424\n",
      "Epoch     : 229\n",
      "Train loss: 0.0027777272535648\n",
      "Valid loss: 0.002895113138347557\n",
      "Epoch     : 230\n",
      "Train loss: 0.002810774354882086\n",
      "Valid loss: 0.00318142127338934\n",
      "Epoch     : 231\n",
      "Train loss: 0.0028148581143952996\n",
      "Valid loss: 0.0030279848313120926\n",
      "Epoch     : 232\n",
      "Train loss: 0.0028280859650167732\n",
      "Valid loss: 0.003000803028448692\n",
      "Epoch     : 233\n",
      "Train loss: 0.0028239840493369424\n",
      "Valid loss: 0.0032796620806906816\n",
      "Epoch     : 234\n",
      "Train loss: 0.002882762616125968\n",
      "Valid loss: 0.0032600318390289006\n",
      "Epoch     : 235\n",
      "Train loss: 0.0028631922136478486\n",
      "Valid loss: 0.0030978716586185662\n",
      "Epoch     : 236\n",
      "Train loss: 0.002805661865700127\n",
      "Valid loss: 0.0030205679209752007\n",
      "Epoch     : 237\n",
      "Train loss: 0.002791129948000348\n",
      "Valid loss: 0.003117849715657543\n",
      "Epoch     : 238\n",
      "Train loss: 0.0027857238906776934\n",
      "Valid loss: 0.003005051861938215\n",
      "Epoch     : 239\n",
      "Train loss: 0.0027144649831491534\n",
      "Valid loss: 0.003032259056029158\n",
      "Epoch     : 240\n",
      "Train loss: 0.0028327526881339704\n",
      "Valid loss: 0.0031219218776661448\n",
      "Epoch     : 241\n",
      "Train loss: 0.00282993026201043\n",
      "Valid loss: 0.0029927505528628386\n",
      "Epoch     : 242\n",
      "Train loss: 0.0027919908435068514\n",
      "Valid loss: 0.003079312632686084\n",
      "Epoch     : 243\n",
      "Train loss: 0.002865410215677099\n",
      "Valid loss: 0.003021898579307789\n",
      "Epoch     : 244\n",
      "Train loss: 0.002869044394592679\n",
      "Valid loss: 0.0029410693180008034\n",
      "Epoch     : 245\n",
      "Train loss: 0.002800183304071051\n",
      "Valid loss: 0.00288691193095802\n",
      "Epoch     : 246\n",
      "Train loss: 0.002836043903731849\n",
      "Valid loss: 0.0029136737388872785\n",
      "Epoch     : 247\n",
      "Train loss: 0.0027958276400517255\n",
      "Valid loss: 0.0030846294778260636\n",
      "Epoch     : 248\n",
      "Train loss: 0.002827803105397709\n",
      "Valid loss: 0.0030083015495266582\n",
      "The validation loss is continuous decrease for 30 time, so training stop\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH_START, NUM_EPOCHS):\n",
    "    \n",
    "        ## 1. Training\n",
    "        model.train()\n",
    "        train_loss = train(model, ema, train_data, device, num_train_data, optimizer) \n",
    "        \n",
    "        ## 2. Evaluating\n",
    "        model.eval()\n",
    "        valid_loss = evaluate(model, valid_data, device, num_valid_data) \n",
    "            \n",
    "        ## 3. Show the result\n",
    "        df.loc[len(df)] = [train_loss, valid_loss]\n",
    "        print_log(f\"Epoch     : {epoch}\")\n",
    "        print_log(f\"Train loss: {train_loss}\")\n",
    "        print_log(f\"Valid loss: {valid_loss}\")\n",
    "        \n",
    "        # After 30 iteration, Random Get  1 epoch of data from validation loader, \n",
    "        # and visualize the reconstructed output\n",
    "        if epoch >= 30 and epoch % 10 == 0:\n",
    "            plt.ioff()\n",
    "            model = model.to(\"cpu\")\n",
    "            diffuse_num_step     = 100\n",
    "            reconstruct_num_step = 100\n",
    "\n",
    "            iterator = iter(valid_data)\n",
    "            original_signal, signal_DWT, label = next(iterator)\n",
    "            rand_num = np.random.randint(0, original_signal.shape[0])\n",
    "            x_0 = original_signal[rand_num][None,:]\n",
    "            x_0_DWT = signal_DWT[rand_num][None,:]\n",
    "            label        = label[rand_num].item()\n",
    "            eps          = torch.randn_like(x_0)\n",
    "            initial_x    = q_sample(x_0, alphas_prod_p_sqrt[diffuse_num_step-1], eps)\n",
    "\n",
    "            x_seq   = p_sample_loop(model, \n",
    "                                    x_0.shape, \n",
    "                                    label   = torch.ones((1), dtype=torch.int64) if label==1 else torch.zeros((1), dtype=torch.int64),\n",
    "                                    n_steps = reconstruct_num_step, \n",
    "                                    initial_x = initial_x,\n",
    "                                    x_DWT = x_0_DWT, \n",
    "                                    device=torch.device(\"cpu\"))  \n",
    "\n",
    "            # Reverse the label\n",
    "            x_seq1   = p_sample_loop(model, \n",
    "                                    x_0.shape, \n",
    "                                    label   = torch.zeros((1), dtype=torch.int64) if label==1 else torch.ones((1), dtype=torch.int64),\n",
    "                                    n_steps = reconstruct_num_step, \n",
    "                                    initial_x = initial_x,\n",
    "                                    x_DWT = x_0_DWT, \n",
    "                                    device=torch.device(\"cpu\"))\n",
    "            fig, axs = plt.subplots(len(AVE_CHANNELS_NAME), 1, figsize=(20, 50))\n",
    "\n",
    "            sample = \"Annotated\" if label == 1 else \"Un-annotated\"\n",
    "            seperate = 1\n",
    "            for i, col in enumerate(AVE_CHANNELS_NAME):\n",
    "                axs[i].set_title(col + sample)\n",
    "                axs[i].plot(x_0[0][i], label=f\"Original Signal\")\n",
    "                axs[i].plot(x_seq[0][0, i] + seperate, label=f\"Noisy Signal_{diffuse_num_step}\")\n",
    "                axs[i].plot(x_seq[-1][0, i] - seperate, label=f\"Reconstructed Signal_{reconstruct_num_step}\")\n",
    "                axs[i].plot(x_seq1[-1][0, i] - seperate*2,label=f\"Reconstructed Signal_{reconstruct_num_step}_if_label_opposite\")\n",
    "                axs[i].legend()\n",
    "\n",
    "            SAVE_PATH = f'{MODEL_FILE_DIRC_WaveGrad}/Epoch_{epoch}_Signal.png'\n",
    "            plt.savefig(SAVE_PATH, transparent=False, facecolor='white')\n",
    "\n",
    "            fig, axs = plt.subplots(len(AVE_CHANNELS_NAME), 1, figsize=(20, 50))\n",
    "            seperate = 1e-4\n",
    "            for i, col in enumerate(AVE_CHANNELS_NAME):\n",
    "                axs[i].set_title(col + sample)\n",
    "                axs[i].plot(inverse_mu_law(x_0[0][i] * NORMALIZE_CONS_2) * NORMALIZE_CONS_1, label=f\"Original Signal\")\n",
    "                axs[i].plot(inverse_mu_law(x_seq[-1][0, i] * NORMALIZE_CONS_2) * NORMALIZE_CONS_1 - seperate, label=f\"Reconstructed Signal_{reconstruct_num_step}\")\n",
    "                axs[i].plot(inverse_mu_law(x_seq1[-1][0, i] * NORMALIZE_CONS_2) * NORMALIZE_CONS_1 - seperate*2,\n",
    "                            label=f\"Reconstructed Signal_{reconstruct_num_step}_if_label_opposite\")\n",
    "                axs[i].legend()\n",
    "\n",
    "            SAVE_PATH1 = f'{MODEL_FILE_DIRC_WaveGrad}/Epoch_{epoch}_Original_Signal.png'\n",
    "            plt.savefig(SAVE_PATH1, transparent=False, facecolor='white')\n",
    "\n",
    "            model = model.to(\"cuda\")\n",
    "            plt.close('all')\n",
    "        \n",
    "        ## 4.4 Plot the loss function\n",
    "        plt.plot(range(len(df[\"Train Loss\"])), df[\"Train Loss\"], label=\"Train Loss\")\n",
    "        plt.plot(range(len(df[\"Train Loss\"])), df[\"Valid Loss\"], label=\"Valid Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{MODEL_FILE_DIRC_WaveGrad}/Loss.png', transparent=False, facecolor='white')\n",
    "        plt.close('all')\n",
    "\n",
    "        ## 4.5. Save model and Stoping criteria\n",
    "        if prev_best_valid_loss > valid_loss:  # If previous validation loss larger than current validation loss (The model is performed better)\n",
    "            state_dict = {\n",
    "                \"model\": model.state_dict(), \n",
    "                \"epoch\":epoch,\n",
    "                \"valid_loss\": valid_loss\n",
    "            }\n",
    "            torch.save(state_dict, f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_best.pt\")\n",
    "            \n",
    "            prev_best_valid_loss = valid_loss  # Previous validation loss = Current validation loss\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            state_dict = {\n",
    "                \"model\": model.state_dict(), \n",
    "                \"epoch\":epoch,\n",
    "                \"valid_loss\": valid_loss\n",
    "            }\n",
    "            torch.save(state_dict, f\"{MODEL_FILE_DIRC_WaveGrad}/Advanced_Diffusion_{epoch}.pt\")\n",
    "        \n",
    "        df.to_csv(f\"{MODEL_FILE_DIRC_WaveGrad}/Loss.csv\", index=False)\n",
    "        \n",
    "        if count == MAX_COUNT:\n",
    "            print_log(f\"The validation loss is continuous decrease for {MAX_COUNT} time, so training stop\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
